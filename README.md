# LLM Evaluation & Reliability Framework

A reproducible, explainable evaluation system for Large Language Model outputs.

## ğŸ¯ Project Goal

Build an engineering-first framework that evaluates LLM reliability by:

- Running deterministic rule-based checks
- Identifying specific failure modes
- Providing explainable scores and reports
- Comparing behavior across models and prompts

## ğŸ—ï¸ Current Status

**Day 2 Complete** âœ…

- 5 working evaluation rules (format + content)
- Modular BaseRule abstract class
- Dimension-based scoring (format_score, content_score)
- Comprehensive test coverage with edge cases
- Bug fixes and improvements

**Rules Implemented:**

1. âœ… Empty Output Detection
2. âœ… JSON Format Validation
3. âœ… Length Constraint Checking
4. âœ… Required Keywords Detection
5. âœ… Forbidden Phrases Detection

## ğŸ“‹ Roadmap

- [x] Day 1: Project setup + basic API
- [x] Day 2: Core rule engine (format, length, keywords)
- [ ] Day 3: Instruction adherence rules
- [ ] Day 4: Prompt injection detection
- [ ] Day 5: Hallucination heuristics
- [ ] Week 2-3: Storage, testing, documentation
- [ ] Week 4+: LLM-as-judge, comparative evaluation

## ğŸ› ï¸ Tech Stack

- **Backend**: FastAPI + Pydantic
- **Testing**: pytest (coming soon)
- **Storage**: JSON/SQLite (coming soon)

## ğŸ“š Learning Goals

This is a capstone project focused on:

- Schema-first API design
- Deterministic evaluation logic
- Failure-mode taxonomy in AI systems
- Building reliable AI infrastructure

## ğŸ“– Documentation

- [Project Statement](docs/PROJECT_STATEMENT.md) - Full project specification
- API Docs: http://127.0.0.1:8000/docs (when server is running)

## ğŸ¤ Contributing

This is a learning project, but feedback welcome via issues!

## ğŸ“ License

MIT License - see LICENSE file
